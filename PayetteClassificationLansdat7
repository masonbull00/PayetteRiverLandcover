//This is the code for analyzing all of landsat 7 for the payette watershed with Scan Line Correction
//It works nearly identically to the Nellie Juan Classifier, but there are some differences
//For more detail of what each line does, check out the Nellie Juan classifier on my Github (masonbull00)

//You will notice that there a lot of commented out 'print' commands. This code takes a while to run
////and printing when it is not necessary really slows it down. Start printing things if you are getting errors to
////see where they are occuring.

//You will need training data for the classes you are analyzing,
////a watershed outline and waterbodies shapfile,
///and a shapefile of the burned areas in the watershed

Map.setCenter(-115.0994, 44.1006, 12);
Map.setOptions({mapTypeId: 'HYBRID'});

// Add ROI and set your date to find imagery
var roi = ee.Geometry.Point(-115.0994, 44.1006); //Payette Watershed
var date_i = '1983-06-01'; // set initial date (YYYY-MM-DD)
var date_f = '2023-10-31'; // Set final date (YYY-MM-DD)

//add an outline of the Payette Watershed to visualize the study area
var styling = {color: 'black', width: 4, fillColor: 'FF000000'};
Map.addLayer(pt_wtrshd.style(styling), null, 'pt_wtrshd');


///////////////////////////////////////////////////////////////////
///////////////Landsat 7 Workflow for the Payette//////////////
///////////////////////////////////////////////////////////////////
//

// add landsat imagery, filter by cloud cover over land, only select spectral bands, and filter by roi and specified date range
var l7 = ee.ImageCollection("LANDSAT/LE07/C02/T1_L2")
  .filterDate(date_i, date_f)
  .filter(ee.Filter.calendarRange(8, 10, 'month'))
  .filterBounds(roi)
  .select('SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B7')
  .filter(ee.Filter.lte('CLOUD_COVER_LAND', 25));
//print('l7', l7);

//add a visualization parameter for the landsat imagery to display in the map if wanted
var l7visParams = {bands: ['B3', 'B2', 'B1'], min:0, max: 20000};

//this applies a QA bitmask to filter out pixels with excessive clouds and shadows
//this function is varied from Spatial Thoughts: https://spatialthoughts.com/2021/08/19/qa-bands-bitmasks-gee/
var qaFunc = function(image){
  var bitwiseExtract = function(input, fromBit, toBit) {
    var maskSize = ee.Number(1).add(toBit).subtract(fromBit);
    var mask = ee.Number(1).leftShift(maskSize).subtract(1);
    return input.rightShift(fromBit).bitwiseAnd(mask);
  };

  var qaBand = image.select('QA_PIXEL');
  var cirrus = bitwiseExtract(qaBand, 14, 15).neq(3);
  //var shadow = bitwiseExtract(qaBand, 10, 11).lte(2); shadow needs taken out as it is too aggressive with dark spots
  var cloud = bitwiseExtract(qaBand, 8, 9).neq(3);
  var bitMask = cirrus.and(cloud);
  return image.mask(bitMask);
};


var checkCRS = l7.first().projection().crs();
//print('l7 crs', checkCRS);

var checkProj = l7.first().projection();


//add SRTM data, clip it to the the Payette, select only the elevation band, and then derive slope and aspect as separate images
var DEM = ee.Image("NASA/NASADEM_HGT/001").clip(pt_wtrshd.geometry()).select('elevation').reproject(checkProj);
var slope = ee.Terrain.slope(DEM);
var aspect = ee.Terrain.aspect(DEM);
//print('Elevation', DEM);
//print('Slope', slope);
//print('Aspect', aspect);


var demClip = pt_wtrshd;

var tmEtmBands = ['SR_B1',   'SR_B2',    'SR_B3',  'SR_B4',  'SR_B5',    'SR_B7'];
var defBands = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2'];


//create a water mask using NHD waterbodies polygons
var ptWaterImg = pt_water.reduceToImage(['fcode'], ee.Reducer.sum()).clip(pt_wtrshd.geometry());
var fireImg = fire.reduceToImage(['Shape_Area'], ee.Reducer.sum()).clip(pt_wtrshd.geometry());
var inverseFire = fireImg.not();
var inverseWater = ptWaterImg.not();


//create a function to apply the water mask to the composited imagery
var applyMask = function(image){
  var i = image.updateMask(inverseFire); 
  return i.updateMask(inverseWater);
};

//create a function to look over imagery and only select the most snow free images,
//this is to account for early snowfall and filter out those images with excess snow

//this function will count the number of snow free pixels
var NDSIth = function(image){
  var getNDSI = image.select('NDSI').gte(0.4).selfMask().rename('NDSIth');
  var pixelCount = getNDSI.reduceRegion({reducer: ee.Reducer.count(), geometry: pt_wtrshd.geometry(), scale: 30, bestEffort: true, maxPixels: 6700000});
  return image.addBands(getNDSI).set('NDSIcount', pixelCount.get('NDSIth'));
};


var l7cor = l7.select(tmEtmBands, defBands);
//print('l7cor', l7cor);

//funtion to compute radians from degrees
function radians(img) {
  return img.toFloat().multiply(Math.PI).divide(180);}
//Get DEM Slope and Aspect in radians
var SLPrads = radians(slope);
var ASPrads = radians(aspect);

//function to do the terrain correction
var scscTCl7 = function(img) {
  var outImage = img.select([]);
  
  //Get the footprint image of the study area
  var footprint = ee.Geometry.Polygon(ee.Number(ee.List(img.get('system:footprint'))));

  //VARIABLES FROM METADATA
  var AZ = ee.Number(img.get('SUN_AZIMUTH'));
  var ZE = ee.Number(img.get('SUN_ELEVATION'));
  var AZ_R = radians(ee.Image(AZ));
  var ZE_R = radians(ee.Image(ZE));
  
  //CALCULATE LOCAL ILLUMINATION, COS OF THE SLOPE, AND COS OF THE ZENITH ANGLE
  var IL = AZ_R.subtract(ASPrads).cos().multiply(SLPrads.sin()).multiply(ZE_R.sin())
  .add(ZE_R.cos().multiply(SLPrads.cos()));
  var cos_ZE = ZE_R.cos();
  var cos_SLP = SLPrads.cos();
  

  //C for the blue band
  var clippedImg = img.addBands(1).addBands(IL);
  var resultBlue = clippedImg.select('constant', 'constant_1', 'blue')
    .reduceRegion({
      reducer: ee.Reducer.linearRegression(2,1),
      geometry: footprint,
      scale: 30,
      maxPixels: 2e9
      });
  var blueC = (ee.Array(resultBlue.get('coefficients')).get([0,0]))
  .divide(ee.Array(resultBlue.get('coefficients')).get([1,0]));
  var blueCorr = ((cos_ZE.multiply(cos_SLP)).add(blueC)).divide(IL.add(blueC));

  
  //C for the green band
  var clippedImg = img.addBands(1).addBands(IL);
  var resultGreen = clippedImg.select('constant', 'constant_1', 'green')
    .reduceRegion({
      reducer: ee.Reducer.linearRegression(2,1),
      geometry: footprint,
      scale: 30,
      maxPixels: 2e9
      });
  var greenC = (ee.Array(resultGreen.get('coefficients')).get([0,0]))
  .divide(ee.Array(resultGreen.get('coefficients')).get([1,0]));
  var greenCorr = ((cos_ZE.multiply(cos_SLP)).add(greenC)).divide(IL.add(greenC));
  
  //C for the red band
  var clippedImg = img.addBands(1).addBands(IL);
  var resultRed = clippedImg.select('constant', 'constant_1', 'red')
    .reduceRegion({
      reducer: ee.Reducer.linearRegression(2,1),
      geometry: footprint,
      scale: 30,
      maxPixels: 2e9
      });
  var redC = (ee.Array(resultRed.get('coefficients')).get([0,0]))
  .divide(ee.Array(resultRed.get('coefficients')).get([1,0]));
  var redCorr = ((cos_ZE.multiply(cos_SLP)).add(redC)).divide(IL.add(redC));
  
  //C for the nir band
  var clippedImg = img.addBands(1).addBands(IL);
  var resultNir = clippedImg.select('constant', 'constant_1', 'nir')
    .reduceRegion({
      reducer: ee.Reducer.linearRegression(2,1),
      geometry: footprint,
      scale: 30,
      maxPixels: 2e9
      });
  var nirC = (ee.Array(resultNir.get('coefficients')).get([0,0]))
  .divide(ee.Array(resultNir.get('coefficients')).get([1,0]));
  var nirCorr = ((cos_ZE.multiply(cos_SLP)).add(nirC)).divide(IL.add(nirC));
  
  //C for the swir1 band
  var clippedImg = img.addBands(1).addBands(IL);
  var resultSwir1 = clippedImg.select('constant', 'constant_1', 'swir1')
    .reduceRegion({
      reducer: ee.Reducer.linearRegression(2,1),
      geometry: footprint,
      scale: 30,
      maxPixels: 2e9
      });
  var swir1C = (ee.Array(resultSwir1.get('coefficients')).get([0,0]))
  .divide(ee.Array(resultSwir1.get('coefficients')).get([1,0]));
  var swir1Corr = ((cos_ZE.multiply(cos_SLP)).add(swir1C)).divide(IL.add(swir1C));
  
  //C for the swir2 band
  var clippedImg = img.addBands(1).addBands(IL);
  var resultSwir2 = clippedImg.select('constant', 'constant_1', 'swir2')
    .reduceRegion({
      reducer: ee.Reducer.linearRegression(2,1),
      geometry: footprint,
      scale: 30,
      maxPixels: 2e9
      });
  var swir2C = (ee.Array(resultSwir2.get('coefficients')).get([0,0]))
  .divide(ee.Array(resultSwir2.get('coefficients')).get([1,0]));
  var swir2Corr = ((cos_ZE.multiply(cos_SLP)).add(swir2C)).divide(IL.add(swir2C));
  
  //IMAGE CORRECTION
  var InImg2 = img.multiply(0.0001);
  
  var blueBand = InImg2.select('blue').multiply(blueCorr);
  //Map.addLayer(blueBand);
  var greenBand = InImg2.select('green').multiply(greenCorr);
  var redBand = InImg2.select('red').multiply(redCorr);
  var nirBand = InImg2.select('nir').multiply(redCorr);
  var swir1Band = InImg2.select('swir1').multiply(swir1Corr);
  var swir2Band = InImg2.select('swir2').multiply(swir2Corr);
  
  var img_TC = img.select().addBands([blueBand, greenBand, redBand, nirBand, swir1Band, swir2Band]);
  //print('img_TC', img_TC)
  
  //ADJUST VALUE RANGE (REFLECTANCE BETWEEN 0 AND 1)
  var img_TC1 = ((img_TC.multiply(10000)).int16());//.multiply(0.0001);
    
  outImage = outImage.addBands(img_TC1
  .select([0,1,2,3,4,5],['blue_TC', 'green_TC', 'red_TC', 'nir_TC', 'swir1_TC', 'swir2_TC']));
  return outImage;
};

var l7corCol = l7cor.map(scscTCl7);
//print('l7 corrected col', l7corCol);

var renameBands = function(image){
  return image.select(['blue_TC', 'green_TC', 'red_TC', 'nir_TC', 'swir1_TC', 'swir2_TC'])
  .rename(['B1', 'B2', 'B3', 'B4', 'B5', 'B7']);
};

var l7renamed = l7corCol.map(renameBands);
//print('RENAMED L7', l7renamed);

// create a function to clip each image in the image collection to the Payette
var clipping = function(image) {
  return image.clip(pt_wtrshd);
};

// Apply clipping function to l8 data
var l7_clip = l7renamed.map(clipping);
//print('l7 clipped', l7_clip);

// create a function to add the DEM elevation, slope, and aspect bands to the landsat images
var addDEM = function(image){
  return image.addBands(DEM).addBands(slope).addBands(aspect);
};

//apply the addDEM function to each image in the collection
var l7DEM = l7_clip.map(addDEM);
//print('l7 + DEM', l7DEM);

//apply the water masking function to the composited imagery
var l7Masked = l7DEM.map(applyMask);
//print('Masked L7 Imagery', l7Masked);

//create a function to generate an NDSI and NDVI on the imagery
var createNDSIl7 = function(image){
  var NDSI = image.normalizedDifference(['B2', 'B5']).rename('NDSI');
  var NDVI = image.normalizedDifference(['B4', 'B3']).rename('NDVI');
  return image.addBands(NDSI).addBands(NDVI);
};

//apply the NDSI and NDVI creation function to the imagery
var l7NDI = l7Masked.map(createNDSIl7);
//print('L7 + Indices', l7NDI);

//create a function to look over imagery and only select the most snow free images,
//this is to account for early snowfall and filter out those images with excess snow

//map the function to get the count of pixels that are snow over the image collection
var l7SnowCount = l7NDI.map(NDSIth);
//print('l7 w snowy pixel count', l7SnowCount);

//this function will pass over the image collection and only keep images with a defined number of 
//snow free pixels
var l7SnowFree = l7SnowCount.filter(ee.Filter.lte('NDSIcount', 30000));
l7SnowFree = l7SnowFree.select('B1', 'B2', 'B3', 'B4', 'B5', 'B7', 
  'elevation', 'slope', 'aspect', 'NDSI', 'NDVI');
//print('l7 snow free images', l7SnowFree);

//grab images for a single year from the snow free collection to make an image from for processing  
//using the new classificaiton technique, then get the median value of the imagery to get to a single 
//image, regardless of there being multiple images in a yearvar l7composite2014 = l7SnowFree.filter(ee.Filter.date('2014-01-01', '2014-12-31')).median();
var l7composite1999 = l7SnowFree.filter(ee.Filter.date('1999-01-01', '1999-12-31')).median();
//print('1999 image composite', l7composite1999);
var l7composite2000 = l7SnowFree.filter(ee.Filter.date('2000-01-01', '2000-12-31')).median();
//print('2000 image composite', l7composite2000);
var l7composite2001 = l7SnowFree.filter(ee.Filter.date('2001-01-01', '2001-12-31')).median();
//print('2001 image composite', l7composite2001);
var l7composite2002 = l7SnowFree.filter(ee.Filter.date('2002-01-01', '2002-12-31')).median();
//print('2002 image composite', l7composite2002);
//var l7composite2003 = l7SnowFree.filter(ee.Filter.date('2003-01-01', '2003-12-31')).median();
//print('2003 image composite', l7composite2003);
//var l7composite2004 = l7SnowFree.filter(ee.Filter.date('2004-01-01', '2004-12-31')).median();
//print('2004 image composite', l7composite2004);
//var l7composite2005 = l7SnowFree.filter(ee.Filter.date('2005-01-01', '2005-12-31')).median();
//print('2005 image composite', l7composite2005);
//var l7composite2006 = l7SnowFree.filter(ee.Filter.date('2006-01-01', '2006-12-31')).median();
//print('2006 image composite', l7composite2006);
//var l7composite2007 = l7SnowFree.filter(ee.Filter.date('2007-01-01', '2007-12-31')).median();
//print('2007 image composite', l7composite2007);
//var l7composite2008 = l7SnowFree.filter(ee.Filter.date('2008-01-01', '2008-12-31')).median();
//print('2008 image composite', l7composite2008);
//var l7composite2009 = l7SnowFree.filter(ee.Filter.date('2009-01-01', '2009-12-31')).median();
//print('2009 image composite', l7composite2009);
//var l7composite2010 = l7SnowFree.filter(ee.Filter.date('2010-01-01', '2010-12-31')).median();
//print('2010 image composite', l7composite2010);
//var l7composite2011 = l7SnowFree.filter(ee.Filter.date('2011-01-01', '2011-12-31')).median();
//print('2011 image composite', l7composite2011);
//var l7composite2012 = l7SnowFree.filter(ee.Filter.date('2012-01-01', '2012-12-31')).median();
//print('2012 image composite', l7composite2012);
//var l7composite2013 = l7SnowFree.filter(ee.Filter.date('2013-01-01', '2013-12-31')).median();
//print('2013 image composite', l7composite2013);
//var l7composite2014 = l7SnowFree.filter(ee.Filter.date('2014-01-01', '2014-12-31')).median();
//print('2014 image composite', l7composite2014);
//var l7composite2015 = l7SnowFree.filter(ee.Filter.date('2015-01-01', '2015-12-31')).median();
//print('2015 image composite', l7composite2015);
//var l7composite2016 = l7SnowFree.filter(ee.Filter.date('2016-01-01', '2016-12-31')).median();
//print('2016 image composite', l7composite2016);
//var l7composite2017 = l7SnowFree.filter(ee.Filter.date('2017-01-01', '2017-12-31')).median();
//print('2017 image composite', l7composite2017);
//var l7composite2018 = l7SnowFree.filter(ee.Filter.date('2018-01-01', '2018-12-31')).median();
//print('2018 image composite', l7composite2018);
//var l7composite2019 = l7SnowFree.filter(ee.Filter.date('2019-01-01', '2019-12-31')).median();
//print('2019 image composite', l7composite2019);
//var l7composite2020 = l7SnowFree.filter(ee.Filter.date('2020-01-01', '2020-12-31')).median();
//print('2020 image composite', l7composite2020);
//var l7composite2021 = l7SnowFree.filter(ee.Filter.date('2021-01-01', '2021-12-31')).median();
//print('2021 image composite', l7composite2021);
//var l7composite2022 = l7SnowFree.filter(ee.Filter.date('2022-01-01', '2022-12-31')).median();
//print('2022 image composite', l7composite2022);



/////////////////////////////////////////
//OK, things are going to get very complicated here.
//This is where the scan line correction begins.
//There are many way to perform this, this method comes from Noel Gorelick on the GEE Dev Forum
//I'll do my best to walk you through this line by line
////////////////////////////////////////

var MIN_SCALE = 1/3; //These are parameters for the moving window to composite the imagery by
var MAX_SCALE = 3;
var MIN_NEIGHBORS = 144;

/* Apply the USGS L7 Phase-2 Gap filling protocol, using a single kernel size. */
//This is essentially a prep phase for the imagery
var GapFill = function(src, fill, kernelSize, upscale) {
  var kernel = ee.Kernel.square(kernelSize * 30, "meters", false); //create the kernel size based on imagery resolution
  
  // Find the pixels common to both scenes.
  var common = src.mask().and(fill.mask()); //this looks for 'tie points' for each image
  var fc = fill.updateMask(common); //these two lines are using the tie point on fill and source images
  var sc = src.updateMask(common);
  //Map.addLayer(common.select(0).mask(common.select(0)), {palette:['000000']}, 'common mask (both exist)', false);

  // Find the primary scaling factors with a regression.
  // Interleave the bands for the regression.  This assumes the bands have the same names.
  var regress = fc.addBands(sc);

  regress = regress.select(regress.bandNames().sort());
  //print(regress,'regress');
  var ratio = 5;
  
  if(upscale) {
    var fit = regress
      .reduceResolution(ee.Reducer.median(), false, 500) //combine images by reducing fill imagery
      .reproject(regress.select(0).projection().scale(ratio, ratio)) //set projection
      .reduceNeighborhood(ee.Reducer.linearFit().forEach(src.bandNames()), kernel, null, false)//reduce surrounding pixels of each previously reduced pixel
      .unmask()
      .reproject(regress.select(0).projection().scale(ratio, ratio)); //finalize source projection
  } else {
    
    var fit = regress
      .reduceNeighborhood(ee.Reducer.linearFit().forEach(src.bandNames()), kernel, null, false)
  }

  var offset = fit.select(".*_offset")//define offset and scale based on above imagery
  var scale = fit.select(".*_scale")

  //Map.addLayer(scale.select('B1_scale'), {min:-2, max:2}, 'scale B1', false)
  
  // Find the secondary scaling factors using just means and stddev
  var Reducer = ee.Reducer.mean().combine(ee.Reducer.stdDev(), null, true)

  if(upscale) {
    var src_stats = src //this is very similar to the function a few lines above 
      .reduceResolution(ee.Reducer.median(), false, 500) //we are getting pixel stats of the source image
      .reproject(regress.select(0).projection().scale(ratio, ratio))
      .reduceNeighborhood(Reducer, kernel, null, false)
      .reproject(regress.select(0).projection().scale(ratio, ratio))

    var fill_stats = fill
      .reduceResolution(ee.Reducer.median(), false, 500)
      .reproject(regress.select(0).projection().scale(ratio, ratio))
      .reduceNeighborhood(Reducer, kernel, null, false)
      .reproject(regress.select(0).projection().scale(ratio, ratio))
  } else {
    var src_stats = src
      .reduceNeighborhood(Reducer, kernel, null, false)

    var fill_stats = fill
      .reduceNeighborhood(Reducer, kernel, null, false)
  }

  var scale2 = src_stats.select(".*stdDev").divide(fill_stats.select(".*stdDev"))
  var offset2 = src_stats.select(".*mean").subtract(fill_stats.select(".*mean").multiply(scale2))

  var invalid = scale.lt(MIN_SCALE).or(scale.gt(MAX_SCALE))//contingency line in case the above stuff can't run
  //Map.addLayer(invalid.select(0).mask(invalid.select(0)), {palette:['550000']}, 'invalid1', false)
  scale = scale.where(invalid, scale2)
  offset = offset.where(invalid, offset2)

  // When all else fails, just use the difference of means as an offset.  
  var invalid2 = scale.lt(MIN_SCALE).or(scale.gt(MAX_SCALE))
  //Map.addLayer(invalid2.select(0).mask(invalid2.select(0)), {palette:['552020']}, 'invalid2', false)
  scale = scale.where(invalid2, 1)
  offset = offset.where(invalid2, src_stats.select(".*mean").subtract(fill_stats.select(".*mean")))

  // Apply the scaling and mask off pixels that didn't have enough neighbors.
  var count = common.reduceNeighborhood(ee.Reducer.count(), kernel, null, true, "boxcar")
  var scaled = fill.multiply(scale).add(offset)
      .updateMask(count.gte(MIN_NEIGHBORS))

  return src.unmask(scaled, true)
}

//This function now gets applied to the actual imagery that we have set up 
//////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////


//now we can perform the correction
var SLCcorrection = function(collection, initialDate, finalDate, imgNumSource, imgNumFill, year){
   // Define a collection filter.
  var colFilter = ee.Filter.and(
    ee.Filter.date(initialDate, finalDate), //timeline to filter over
    ee.Filter.bounds(pt_wtrshd), //imagery bounds
    //ee.Filter.eq('WRS_PATH', 40), //This can be helpful if you on the edge of a few Landsat scenes (which the payette is) 
    //ee.Filter.eq('WRS_ROW', 29),  //Leave this commented until you are confident you know where the scenes lie and interact
    ee.Filter.lt('CLOUD_COVER', 10),//the rest of these are just image quality selectors
    ee.Filter.lt('GEOMETRIC_RMSE_MODEL', 10),
    ee.Filter.or(
      ee.Filter.eq('IMAGE_QUALITY', 9),
      ee.Filter.eq('IMAGE_QUALITY_OLI', 9)
    ));
  // Filter collections and prepare them for merging.
  var l7images = collection.filter(colFilter); //filter the imagery
  //print(l7images,'Landsat 7 ' + year);
  
  var listOfImages = l7images.toList(l7images.size()); //convert the image collection to a list
  print(listOfImages,'listOfImages ' + year);
  var firstImage  = ee.Image(listOfImages.get(imgNumSource)); //set the first (source) image
  var secondImage = ee.Image(listOfImages.get(imgNumFill));//set second (fill) image
  var lastImage   = ee.Image(listOfImages.get(listOfImages.length().subtract(1)));
  
  var source    = ee.Image(listOfImages.get(imgNumSource));
  
  var fill      = ee.Image(listOfImages.get(imgNumFill));
  
  
  // take images from the same WRS path and +/- N months
  var fillImages = l7images
    //.filter(ee.Filter.eq('WRS_PATH', firstImage.get('WRS_PATH'))) //this is essentially the same as WRS selection above
    .filterDate(source.date().advance(-3, 'month'), source.date().advance(3, 'month'));//look for close dates by month
  
    // check if we're getting images which fill SLC-OFF gaps by comparing masks
    fillImages = fillImages.map(function(i) {
      var xorMask = i.select(0).mask().and(source.select(0).mask().not());
      //look for images with the least amount of missing data
      var area = ee.Image.pixelArea().mask(xorMask).reduceRegion({
        reducer: ee.Reducer.sum(), 
        geometry: source.geometry(), 
        scale: 300, 
      }).values().get(0);
      
      return i.set({ gapArea: area });
    });
  
  var fill = fillImages.sort('gapArea', false).first(); //I don't like to automatically select based on area, so I comment this out
  ////this is totally fine to use however, I just prefer to do it manually
  
  var vis = {min:0, max:20000, bands:["B3", "B2", "B1"]};
  
  //Map.addLayer(fill,    vis, "1 " + year, 0);
  //Map.addLayer(source,  vis, "2 " + year, 0);
  print(source,'First '+ year);
  print(fill,'Second '+ year);
  
  //Map.addLayer(fill,    vis, "fill "+ year, false);
  //Map.addLayer(source,  vis, "destination " + year, false);
  var result = GapFill(source, fill, 30, false); //this applies the gap filling above to the 2 images we create in this function
  //Map.addLayer(result,  vis, "filled " + year, false);

  var resultUpscaled = GapFill(source, fill, 30, true);//create the upscaled gapfilled image
  Map.addLayer(resultUpscaled,  vis, "filled (upscaled) " + year, 0);
  print('result upscaled ' + year, resultUpscaled);
  
return resultUpscaled;
};

//here we actually decide what images get the gap filling. 
//I have some images commented out that only have one image, so we can't gap fill
//I also leave out the images with no useful imagery available
//I find it is easiest to focus on one year at a time and go through it's images in the image list 
////to see which images will fit the best together
//You can change the 3rd and 2nd to last number to manually select which images to use in the gap filling
var slc2003 = SLCcorrection(l7SnowFree, '2003-08-01', '2003-10-31', 1, 2, '2003');
var slc2004 = SLCcorrection(l7SnowFree, '2004-08-01', '2004-10-31', 2, 1, '2004');
var slc2005 = SLCcorrection(l7SnowFree, '2005-08-01', '2005-10-31', 1, 2, '2005');
//var slc2006 = SLCcorrection(l7SnowFree, '2006-08-01', '2006-10-31', 0, 1, '2006');   //one img
var slc2007 = SLCcorrection(l7SnowFree, '2007-08-01', '2007-10-31', 2, 1, '2007');
var slc2008 = SLCcorrection(l7SnowFree, '2008-08-01', '2008-10-31', 0, 1, '2008');   
//var slc2009 = SLCcorrection(l7SnowFree, '2009-08-01', '2009-10-31', 0, 1, '2009');   //one img
var slc2010 = SLCcorrection(l7SnowFree, '2010-08-01', '2010-10-31', 2, 1, '2010');
var slc2011 = SLCcorrection(l7SnowFree, '2011-08-01', '2011-10-31', 0, 1, '2011');
var slc2012 = SLCcorrection(l7SnowFree, '2012-08-01', '2012-10-31', 0, 1, '2012');
//var slc2013 = SLCcorrection(l7SnowFree, '2013-08-01', '2013-10-31', 0, 1, '2013'); //not great but only 2 images
var slc2014 = SLCcorrection(l7SnowFree, '2014-08-01', '2014-10-31', 0, 1, '2014');
var slc2015 = SLCcorrection(l7SnowFree, '2015-08-01', '2015-10-31', 0, 1, '2015');
//var slc2016 = SLCcorrection(l7SnowFree, '2016-08-01', '2016-10-31', 0, 1, '2016');   //one img
//var slc2017 = SLCcorrection(l7SnowFree, '2017-08-01', '2017-10-31', 0, 1, '2017');   //one img
var slc2018 = SLCcorrection(l7SnowFree, '2018-08-01', '2018-10-31', 0, 1, '2018');
var slc2019 = SLCcorrection(l7SnowFree, '2019-08-01', '2019-10-31', 8, 1, '2019');  //Need to be played with
var slc2020 = SLCcorrection(l7SnowFree, '2020-08-01', '2020-10-31', 4, 2, '2020');  //Need to be played with
var slc2021 = SLCcorrection(l7SnowFree, '2021-08-01', '2021-10-31', 1, 2, '2021');  //Need to be played with
var slc2022 = SLCcorrection(l7SnowFree, '2022-08-01', '2022-10-31', 0, 2, '2022');  //Need to be played with
//var slc2023 = SLCcorrection(l7SnowFree, '2023-08-01', '2023-10-31', 1, 2, '2023'); //too cloudy

//define the important bands to use in the classification 
var l7bands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B7', 'elevation',
  'slope', 'aspect', 'NDSI', 'NDVI'];

//create an empty list to add accuracies to after classification
var accuraciesL7 = ee.List([]);

  //This class list includes water, but because we have a water mask this class can be removed
var classesNW = classes.filter(ee.Filter.inList('Class', [1, 2, 3, 4, 5, 6]));//.filter(
  //ee.Filter.lte('random', 0.8));
var classes84NW = classes84.filter(ee.Filter.inList('Class', [1, 2, 3, 4, 5, 6]));//.filter(
  //ee.Filter.lte('random', 0.8));
  
  
//This function will create training data for each image using the previously created split
//in the class data 
function classifyDataL7(image, points, year) {
  var trainingPoints = points.filter(ee.Filter.lte('random', 0.8));
  var validationPoints = points.filter(ee.Filter.gt('random', 0.8));
  
  var NDVI = image.select('NDVI');
  
  //sample the training data over the imagery
  var training = image.reduceRegions({
    collection: trainingPoints,
    reducer: ee.Reducer.first(),
    scale: 30
  }).filter(ee.Filter.neq('B1', null)).filter(ee.Filter.neq('B2', null))
  .filter(ee.Filter.neq('B3', null)).filter(ee.Filter.neq('B4', null))
  .filter(ee.Filter.neq('B5', null)).filter(ee.Filter.neq('B6', null))
  .filter(ee.Filter.neq('B7', null))
  .filter(ee.Filter.neq('NDVI', null)).filter(ee.Filter.neq('NDSI', null));
  
  //create and train an empty classifer
  var classifier = ee.Classifier.smileRandomForest(250)
  .train({
    features: training,
    classProperty: 'Class',
    inputProperties: image.bandNames()
  });
  
    //get classifier expanations
  var importance = ee.Dictionary(classifier.explain()
  .get('importance', 'outOfBagErrorEstimate'));
  
  //print('importance ' + year, importance);
  var totalImportance = importance.values().reduce(ee.Reducer.sum());
  
  var importancePercentage = importance.map(function (band, importance) {
  return ee.Number(importance).divide(totalImportance).multiply(100)}); 
  
  //print('importance Percentage ' + year, importancePercentage);
  
  //classify the images
  var classifiedImage = image.classify(classifier);
  
  var classedVisParams = {min: 1, max: 6, 
  'palette': ['45414E', '0E5B07', '16E810', '152AF5', '7D5108', 'ffd37f']
  };
  //add the classified layer to the map
  Map.addLayer(classifiedImage, classedVisParams, 'L7 ' + year, 0);
  //print('classified Image '+ year, classifiedImage);
  
  //use the map to get accuracy metrics from validation data created earlier
  var testing = classifiedImage.sampleRegions({
    collection: validationPoints,
    properties: ['Class'],
    scale: 30
  });
      //find out which points are misclassified and display them
  var misclassifiedl7 = testing.filter(ee.Filter.notEquals({ 
    leftField: 'Class', rightField: 'classification'
  }));
  
      
  var pointStyle = ee.Dictionary({
    1: {color: 'red', fillColor: '45414E', pointShape: 'circle', pointSize: 5},
    2: {color: 'red', fillColor: '0E5B07', pointShape: 'circle', pointSize: 5},
    3: {color: 'red', fillColor: '16E810', pointShape: 'circle', pointSize: 5},
    4: {color: 'red', fillColor: '152AF5', pointShape: 'circle', pointSize: 5},
    5: {color: 'red', fillColor: '7D5108', pointShape: 'circle', pointSize: 5},
    6: {color: 'red', fillColor: 'ffd37f', pointShape: 'circle', pointSize: 5}
  });
  
  misclassifiedl7 = misclassifiedl7.map(function(feature){
    return feature.set('style', pointStyle.get(feature.get('Class')));
  });
  //print('Misclassified Points ' + year, misclassifiedl7);
  Map.addLayer(misclassifiedl7.style({styleProperty: 'style'}), {}, 'Misclassified Points ' + year, 0);
  
  //create a confusion matrix for the data
  var testingConfusionMatrix = testing.errorMatrix('Class', 'classification');
  //print the matrices
  print('confusion matrix l8 ' + year, testingConfusionMatrix);
  print('test accuracy l8 ' + year, testingConfusionMatrix.accuracy());
  //print('Users Accuracy '+year, testingConfusionMatrix.consumersAccuracy());
  //print('Producers Accuracy '+year, testingConfusionMatrix.producersAccuracy());
  //print('Kappa '+year, testingConfusionMatrix.kappa());
  
  var accuracy = testingConfusionMatrix.accuracy();
  accuraciesL7 = accuraciesL7.add(accuracy);
  
    var histo = ui.Chart.image.histogram({image: classifiedImage,
                                        region: pt_wtrshd.geometry(),
                                        scale: 30
                                          })//.setSeriesNames(['Rock', 'Dense Forest', 'Tran Forest', 'Riparian', 'Veg Talus', 'Tree Talus'])
  .setOptions({hAxis: {title: 'Class'}, 
  title: 'Count of Classification Occurence for '+year,
  colors: ['45414E', '0E5B07', '16E810', '152AF5', '7D5108', 'ffd37f']});
  print('class histogram ' + year, histo);
  
  //export the generate error matrix to Google Drive in a specified folder
  //(the matrix needs to be converted to a featureCollection first)

  //export the generated error matrix to Google Drive in a specified folder
  //(the matrix needs to be converted to a featureCollection first)
  
  //this step converts the error matrix into an array, and then a callable feature
  //var exportMatrix = ee.Feature(null, {matrix: testingConfusionMatrix.array()});
  //Export.table.toDrive({
  //  collection: ee.FeatureCollection(exportMatrix),
  //  description: 'L7_Confusion_Matrix_for_' + year,
  //  folder: 'ErrorMatrix',
  //  fileNamePrefix: 'L7Matrix' + year,
  //  fileFormat: 'CSV'
  //});
  //
  ////export the feature of class areas 
  //Export.table.toDrive({
  //  collection: ee.FeatureCollection(classAreaFeature),
  //  description: 'L7_Classes_Areas_for_' + year,
  //  folder: 'ClassArea',
  //  fileNamePrefix: 'L7ClassArea' + year,
  //  fileFormat: 'CSV'
  //});
  //
  ////export imagery to google drive
  //Export.image.toDrive({
  //  image: classifiedImage,
  //  description: 'L7_FM_Classified_Image' + year,
  //  folder: 'PayetteGEEOutputs',
  //  fileNamePrefix: 'L7_PT_FM_ClassImg' + year,
  //  scale: 30,
  //  fileFormat: 'GeoTIFF',
  //  region: pt_wtrshd.geometry(),
  //  crs: 'EPSG:32611'
  //});
  ////export ndvi to google drive
  //Export.image.toDrive({
  //  image: NDVI,
  //  description: 'L7_FM_NDVI' + year,
  //  folder: 'PayetteGEEOutputs',
  //  fileNamePrefix: 'L7_PTFM_NDVI' + year,
  //  scale: 30,
  //  maxPixels: 9e7,
  //  fileFormat: 'GeoTIFF',
  //  region: pt_wtrshd.geometry(),
  //  crs: 'EPSG:32611'
  //});
  ////Export a classified image to an asset
  //Export.image.toAsset({
  //  image: classifiedImage,
  //  description: 'classifiedL7' + year + 'toAsset',
  //  assetId: 'projects/boise-state-bull-thesis-work/assets/PT_FM_classifiedL7' + year,
  //  region: pt_wtrshd.geometry(),
  //  scale: 30,
  //  crs: 'EPSG:32611'
  //});

};


//run the classifier function on the composited imagery from each year,
//outputs an image on the map of the classification as well as a confusion
//matrix and accuracy in the console. Also adds in the corresponding year's Landsat image
Map.addLayer(l7composite1999, l7visParams, 'Landsat 7 1999', 0);
classifyDataL7(l7composite1999, classes84NW, '1999');
Map.addLayer(l7composite2000, l7visParams, 'Landsat 7 2000', 0);
classifyDataL7(l7composite2000, classes84NW, '2000');
Map.addLayer(l7composite2001, l7visParams, 'Landsat 7 2001', 0);
classifyDataL7(l7composite2001, classes84NW, '2001');
Map.addLayer(l7composite2002, l7visParams, 'Landsat 7 2002', 0);
classifyDataL7(l7composite2002, classes84NW, '2002');
Map.addLayer(ee.Image(slc2003), l7visParams, 'Landsat 7 2003', 0);
classifyDataL7(ee.Image(slc2003), classesNW, '2003');
Map.addLayer(ee.Image(slc2004), l7visParams, 'Landsat 7 2004', 0);
classifyDataL7(ee.Image(slc2004), classesNW, '2004');
Map.addLayer(ee.Image(slc2005), l7visParams, 'Landsat 7 2005', 0);
classifyDataL7(ee.Image(slc2005), classesNW, '2005');
//Map.addLayer(ee.Image(slc2006), l7visParams, 'Landsat 7 2006', 0);//bad imagery
//classifyDataL7(ee.Image(slc2006), classesNW, '2006');
Map.addLayer(ee.Image(slc2007), l7visParams, 'Landsat 7 2007', 0);
classifyDataL7(ee.Image(slc2007), classesNW, '2007');
Map.addLayer(ee.Image(slc2008), l7visParams, 'Landsat 7 2008', 0);
classifyDataL7(ee.Image(slc2008), classesNW, '2008');
//Map.addLayer(ee.Image(slc2009), l7visParams, 'Landsat 7 2009', 0);//bad imagery
//classifyDataL7(ee.Image(slc2009), classesNW, '2009');
Map.addLayer(ee.Image(slc2010), l7visParams, 'Landsat 7 2010', 0);
classifyDataL7(ee.Image(slc2010), classesNW, '2010');
Map.addLayer(ee.Image(slc2011), l7visParams, 'Landsat 7 2011', 0);
classifyDataL7(ee.Image(slc2011), classesNW, '2011');
Map.addLayer(ee.Image(slc2012), l7visParams, 'Landsat 7 2012', 0);
classifyDataL7(ee.Image(slc2012), classesNW, '2012');
//Map.addLayer(ee.Image(slc2013), l7visParams, 'Landsat 7 2013', 0);//bad imagery
//classifyDataL7(ee.Image(slc2013), classesNW, '2013');
Map.addLayer(ee.Image(slc2014), l7visParams, 'Landsat 7 2014', 0);
classifyDataL7(ee.Image(slc2014), classesNW, '2014');
Map.addLayer(ee.Image(slc2015), l7visParams, 'Landsat 7 2015', 0);
classifyDataL7(ee.Image(slc2015), classesNW, '2015');
//Map.addLayer(ee.Image(slc2016), l7visParams, 'Landsat 7 2016', 0);//bad imagery
//classifyDataL7(ee.Image(slc2016), classesNW, '2016');
//Map.addLayer(ee.Image(slc2017), l7visParams, 'Landsat 7 2017', 0);//bad imagery
//classifyDataL7(ee.Image(slc2017), classesNW, '2017');
Map.addLayer(ee.Image(slc2018), l7visParams, 'Landsat 7 2018', 0);
classifyDataL7(ee.Image(slc2018), classesNW, '2018');
Map.addLayer(ee.Image(slc2019), l7visParams, 'Landsat 7 2019', 0);
classifyDataL7(ee.Image(slc2019), classesNW, '2019');
Map.addLayer(ee.Image(slc2020), l7visParams, 'Landsat 7 2020', 0);
classifyDataL7(ee.Image(slc2020), classesNW, '2020');
Map.addLayer(ee.Image(slc2021), l7visParams, 'Landsat 7 2021', 0);
classifyDataL7(ee.Image(slc2021), classesNW, '2021');
Map.addLayer(ee.Image(slc2022), l7visParams, 'Landsat 7 2022', 0);
classifyDataL7(ee.Image(slc2022), classesNW, '2022');
//Map.addLayer(ee.Image(slc2023), l7visParams, 'Landsat 7 2023', 0);//bad imagery
//classifyDataL7(ee.Image(slc2023), classesNW, '2023');

var listOfYearsL7 = ['1999', '2000', '2001', '2002', '2003',
                     '2004', '2005', '2007', '2008', '2010', '2011', '2012',
                     '2014', '2015', '2018', '2019', '2020', '2021', '2022'];
print('list of accuracies Landsat 7', accuraciesL7);
var accuracyChartL7 = ui.Chart.array.values({array: accuraciesL7, axis: 0, xLabels: listOfYearsL7})
                    .setOptions({title: 'Accuracy of Classification Over Time',
                      trendlines: {0:{}}
                    });
print('accuracy chart L7', accuracyChartL7);
